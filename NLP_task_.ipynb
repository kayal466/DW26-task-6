{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kayal466/DW26-task-6/blob/main/NLP_task_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc5d7Ks-arHa",
        "outputId": "e4c891b1-8f99-4fdb-9404-ff7ae941a408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting chardet==3.*\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hstspreload\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx==0.13.3->googletrans) (2022.12.7)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.8/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15735 sha256=62290275b3add4afbb4ed6aa18d049f228f6304bb2f80d23309114a4d8e5eba8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/ce/9b/d51de1064911d42480ab6b57fc943ee36572441f27546354e2\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, sniffio, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install googletrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWVmQ3wWbD8C",
        "outputId": "72f258f4-2e0f-48c7-8736-7a5da27102eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'af': 'afrikaans', 'sq': 'albanian', 'am': 'amharic', 'ar': 'arabic', 'hy': 'armenian', 'az': 'azerbaijani', 'eu': 'basque', 'be': 'belarusian', 'bn': 'bengali', 'bs': 'bosnian', 'bg': 'bulgarian', 'ca': 'catalan', 'ceb': 'cebuano', 'ny': 'chichewa', 'zh-cn': 'chinese (simplified)', 'zh-tw': 'chinese (traditional)', 'co': 'corsican', 'hr': 'croatian', 'cs': 'czech', 'da': 'danish', 'nl': 'dutch', 'en': 'english', 'eo': 'esperanto', 'et': 'estonian', 'tl': 'filipino', 'fi': 'finnish', 'fr': 'french', 'fy': 'frisian', 'gl': 'galician', 'ka': 'georgian', 'de': 'german', 'el': 'greek', 'gu': 'gujarati', 'ht': 'haitian creole', 'ha': 'hausa', 'haw': 'hawaiian', 'iw': 'hebrew', 'he': 'hebrew', 'hi': 'hindi', 'hmn': 'hmong', 'hu': 'hungarian', 'is': 'icelandic', 'ig': 'igbo', 'id': 'indonesian', 'ga': 'irish', 'it': 'italian', 'ja': 'japanese', 'jw': 'javanese', 'kn': 'kannada', 'kk': 'kazakh', 'km': 'khmer', 'ko': 'korean', 'ku': 'kurdish (kurmanji)', 'ky': 'kyrgyz', 'lo': 'lao', 'la': 'latin', 'lv': 'latvian', 'lt': 'lithuanian', 'lb': 'luxembourgish', 'mk': 'macedonian', 'mg': 'malagasy', 'ms': 'malay', 'ml': 'malayalam', 'mt': 'maltese', 'mi': 'maori', 'mr': 'marathi', 'mn': 'mongolian', 'my': 'myanmar (burmese)', 'ne': 'nepali', 'no': 'norwegian', 'or': 'odia', 'ps': 'pashto', 'fa': 'persian', 'pl': 'polish', 'pt': 'portuguese', 'pa': 'punjabi', 'ro': 'romanian', 'ru': 'russian', 'sm': 'samoan', 'gd': 'scots gaelic', 'sr': 'serbian', 'st': 'sesotho', 'sn': 'shona', 'sd': 'sindhi', 'si': 'sinhala', 'sk': 'slovak', 'sl': 'slovenian', 'so': 'somali', 'es': 'spanish', 'su': 'sundanese', 'sw': 'swahili', 'sv': 'swedish', 'tg': 'tajik', 'ta': 'tamil', 'te': 'telugu', 'th': 'thai', 'tr': 'turkish', 'uk': 'ukrainian', 'ur': 'urdu', 'ug': 'uyghur', 'uz': 'uzbek', 'vi': 'vietnamese', 'cy': 'welsh', 'xh': 'xhosa', 'yi': 'yiddish', 'yo': 'yoruba', 'zu': 'zulu'}\n"
          ]
        }
      ],
      "source": [
        "import googletrans\n",
        "print(googletrans.LANGUAGES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OCfxoYSrcY9a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "hJbLUL-NclGr",
        "outputId": "878b0174-b02b-4e81-d627-ad43b46bc7bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  English words/sentences\n",
              "0                                                     Hi.\n",
              "1                                                    Run!\n",
              "2                                                    Run!\n",
              "3                                                    Who?\n",
              "4                                                    Wow!\n",
              "...                                                   ...\n",
              "171222  In the USA, opioids cause more deaths than tra...\n",
              "171223  Is it true that Boston is a popular destinatio...\n",
              "171224  Is there any way you can un-shrink a T-shirt t...\n",
              "171225  It costs more to mint a penny than the penny i...\n",
              "171226  It doesn't matter to us if you take a photo fr...\n",
              "\n",
              "[171227 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c97114a-27d9-4047-a55f-85ef255af4e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English words/sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Who?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Wow!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171222</th>\n",
              "      <td>In the USA, opioids cause more deaths than tra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171223</th>\n",
              "      <td>Is it true that Boston is a popular destinatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171224</th>\n",
              "      <td>Is there any way you can un-shrink a T-shirt t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171225</th>\n",
              "      <td>It costs more to mint a penny than the penny i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171226</th>\n",
              "      <td>It doesn't matter to us if you take a photo fr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>171227 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c97114a-27d9-4047-a55f-85ef255af4e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c97114a-27d9-4047-a55f-85ef255af4e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c97114a-27d9-4047-a55f-85ef255af4e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "word = pd.read_csv('/content/English (1).csv')\n",
        "word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYuUYCNyw8-f",
        "outputId": "3b78cd2b-9a54-4dd3-c10f-01bd183f2816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 171227 entries, 0 to 171226\n",
            "Data columns (total 1 columns):\n",
            " #   Column                   Non-Null Count   Dtype \n",
            "---  ------                   --------------   ----- \n",
            " 0   English words/sentences  171227 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 1.3+ MB\n"
          ]
        }
      ],
      "source": [
        "word.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "jajrryNqxyCh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DEA0vFr9x5QS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word= word['English words/sentences'].lower().astype(str)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('cmw.1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "y95KCAcP4SxZ",
        "outputId": "35a0d056-3985-4d8e-efb6-92572bf1512a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-82c0e444b536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'English words/sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cmw.1.4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens=nltk.sent_tokenize(word['English words/sentences'].apply(str))\n",
        "word_tokens=nltk.word_tokenize(word['English words/sentences'].apply(str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "aHJIOAyo4Mz0",
        "outputId": "bee71948-156c-4def-be73-508041f4f9b8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-1b177df6a2be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'English words/sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mword_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'English words/sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \"\"\"\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \"\"\"\n\u001b[1;32m   1420\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0mbefore_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;31m# Ignore matches that have already been captured by matches to the right of this match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbefore_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j96cH0g6WnA"
      },
      "source": [
        "# Language Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPEOXSLI55xn",
        "outputId": "3e860ef4-fff1-4059-d0f3-bf3cd95ba3e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.8/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.8/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic-zZE3SfKy1",
        "outputId": "8156838d-9138-4568-9bfa-a34467794d89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lied\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from textblob import TextBlob\n",
        "word_new= TextBlob('song')\n",
        "new=word_new.translate(from_lang='en',to='de')\n",
        "print(new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHFeUWPHtcMo"
      },
      "source": [
        "# Resume Screening Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zeb6qg6pCvY",
        "outputId": "36251589-d893-4e7e-8481-7c9caaecb62b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=583a34aef9d2d177f953efa512e6a0935194b98aa5a26c86fbb3971676a837a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/f0/2c/81637d42670985178b77df6d41b9b6c6dc18c94818447414b9\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install docx2txt\n",
        "import docx2txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2sl0es0pdAB"
      },
      "outputs": [],
      "source": [
        "desc=docx2txt.process('/content/Dendrite.ai - JD - DS Intern.docx')\n",
        "resume=docx2txt.process('/content/CURRICULAM VITAE (1).docx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEKnzFhVqq-K",
        "outputId": "fe8dcf4d-5bef-4a93-b5d7-0449c0184fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CURRICULAM VITAE\n",
            "\n",
            "Name\n",
            "\n",
            ":     S.Kayalvizhi,MA,Mphil.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Date of Birth\n",
            "\n",
            ":     26th July 1996\n",
            "\n",
            "\n",
            "\n",
            "Nationality\n",
            "\n",
            ":     Indian\n",
            "\n",
            "\n",
            "\n",
            "Marital Status\n",
            "\n",
            ":     Unmarried\n",
            "\n",
            "\n",
            "\n",
            "Language Known\n",
            "\n",
            ":     English & Tamil\n",
            "\n",
            "\n",
            "\n",
            "Mobile\n",
            "\n",
            ":     +91 9159026740\n",
            "\n",
            "\n",
            "\n",
            "Email\n",
            "\n",
            ":     ammukayal73@gmail.com\n",
            "\n",
            "Permanent Address\n",
            "\n",
            ":     73,Kaliyamman Kovil Street, S.Pudur(PO)\n",
            "\n",
            "       Pin- 612201\n",
            "\n",
            "      Thiruvidaimaruthur(TK),\n",
            "\n",
            "      Thanjavur(DT),Tamilnadu.\n",
            "\n",
            "WORKING EXPERIENCE:\n",
            "\n",
            "Worked as a Lecturer in AVC Polytechnic College – Mannampandal, Mayiladuthurai. From 02.07.2019 to 31.07.2022 (3 Years)\n",
            "\n",
            "CERTIFICATION:\n",
            "\n",
            "\n",
            "\n",
            "DK70004F14Sc62R995 – Master Data Science\n",
            "\n",
            "Credential Link    -www.guvi.in/verify-certificate?id=DK70004F14Sc62R995\n",
            "\n",
            "Credential ID  - Data Analytics Using Pandas - 61985z257c92bVei88\n",
            "\n",
            "Credential Link  -www.guvi.in/verify-certificate?id=61985z257c92bVei88\n",
            "\n",
            "Credential ID - Machine Learning  - 1017T86U1753LNP9t6O84\n",
            "\n",
            "Credential Link – www.guvi.in/verify-certificate?id=7T86U1753LNP9t6O84\n",
            "\n",
            "Credential ID – Python - 1KF53pR62613J0BD40\n",
            "\n",
            "Credential Linkwww.guvi.in/verify-certificate?id=1KF53pR62613J0BD40\n",
            "\n",
            "Credential ID – PowerBI - pbg8t6F3166K1M606u\n",
            "\n",
            "Credential Link - www.guvi.in/verify-certificate?id=pbg8t6F3166K1M606u\n",
            "\n",
            "Credential ID – MongoDB - 5O2V1066P9lEU7Xe56\n",
            "\n",
            "Credential Link- www.guvi.in/verify-certificate?id=5O2V1066P9lEU7Xe56\n",
            "\n",
            "\t\tAcademic Qualification:\n",
            "\n",
            "Master of DataScience (2021 September – 2022 March) \n",
            "\n",
            "\n",
            "\n",
            "Discipline  \t\t:       \tMDS\n",
            "\n",
            "Institute \t\t: \tGUVI - IITM\n",
            "\n",
            "\n",
            "\n",
            "Project Name:  E-Commerce Customer Segmentation.\n",
            "\n",
            "Description:\n",
            "\n",
            "\n",
            "\n",
            "\tThe Idea of the project was dividing the customers into a group based on their needs and expectation.\n",
            "\n",
            "\tThe dataset contains 5,41,803 values. The steps of the project was imported an essential libraries and i have  done Eda on the entire dataset found the null values dropped it because it was very less. To found the similarity between customers applied NLP on the description column of the dataset.\n",
            "\n",
            "\tApplied nlp process such as removed the punctuation marks, checked upper case and lower case, word tokenization, sentence tokenization, stop word removal, lemmatization, tagged the parts of speech and applied TFIDF. Finally found the best value of K.  With the help of k visualized the clusters in k means algorithm and then grouped the data based on their frequency. \n",
            "\n",
            "Master of Philosophy (2018 – 2019) \n",
            "\n",
            "\n",
            "\n",
            "Discipline  \t\t:       \tM.phil (English)\n",
            "\n",
            "Institute \t\t: \tPrist University\n",
            "\n",
            "University\t\t: \tPrist University, Vallam,Thanjavur.\n",
            "\n",
            "\tClass\t:\tFirst Class\n",
            "\n",
            "\tPercentage                :\t\n",
            "\n",
            "Master of Arts (2016 - 2018) (Full time)\n",
            "\n",
            "\n",
            "\n",
            "Discipline  \t\t:       \tMA (English)\n",
            "\n",
            "Institute \t\t: \tGovernment college for Women, Kumbakonam\n",
            "\n",
            "University\t:\tBharathidasan University, Trichy\n",
            "\n",
            "Class\t\t\t: \tFirst Class\n",
            "\n",
            "Percentage\t\t: \t71 %\n",
            "\n",
            "\n",
            "\n",
            "Bachelor of Arts (2013 - 2016) (Full time)\n",
            "\n",
            "\n",
            "\n",
            "Discipline \t: \tBA (English)\n",
            "\n",
            "Institute\t\t:\tAnnai Vailankanni Arts and Science college                   \t                                           \n",
            "\n",
            "University   \t: \tBharathidasan University, Trichy\n",
            "\n",
            "Class            \t: \tFirst Class\n",
            "\n",
            "Percentage   \t: \t70 %\n",
            "\n",
            "HSC (passed out-2013) \n",
            "\n",
            "Medium \t\t: \tState Board \n",
            "\n",
            "Percentage\t\t:\t74.08 % \n",
            "\n",
            "School \t\t: \tSaraswathi Padasala Higher Secondary School,\n",
            "\n",
            "\t\t\t\t\t\tKumbakonam\n",
            "\n",
            "SSLC (passed out-2011) \n",
            "\n",
            "Medium \t\t:\t State Board \n",
            "\n",
            "Percentage \t\t:\t 90% \n",
            "\n",
            "School \t\t:\tS.Pudur High School, S.Pudur.\n",
            "\n",
            "\n",
            "\n",
            "I hereby declare that the above given information is correct and complete to the best of my knowledge.\n",
            "\n",
            "Yours sincerely\n",
            "\n",
            "(S.Kayalvizhi)\n"
          ]
        }
      ],
      "source": [
        "print(resume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJJpk_XLqtad",
        "outputId": "a170c190-4e86-447c-fe64-9ecb4a5cdf35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Company Profile: Dendrite, as it stands, is an end-to-end statistical learning platform that has three products - Visualistic(BI), Statlab(ML), and Newster(News). There are several problems and challenges that we would like to tackle head-on. These I have faced personally and heard through my conversations with banking and investment banking colleagues. We aim to make Dendrite a full-fledged research terminal that enables retail and institutional data wranglers. \n",
            "\n",
            " \n",
            "\n",
            " Job code: DSJAN2301 \n",
            "\n",
            " Company Name: Dendrite.ai \n",
            "\n",
            " Company Website: https://dendrite.ai/ \n",
            "\n",
            " Job Role: Data Scientist Intern \n",
            "\n",
            " Nature of Job: Internship \n",
            "\n",
            " Stipend: 7k - 14k (Based on Interview performance) \n",
            "\n",
            " Type Job (WFH / WFO): WFH \n",
            "\n",
            " Location: Remote \n",
            "\n",
            " Openings: 2 \n",
            "\n",
            " Experience (Years): Freshers \n",
            "\n",
            " No of Rounds of Interview: 3 \n",
            "\n",
            "Screening Round \n",
            "\n",
            "Technical Round \n",
            "\n",
            "HR Round \n",
            "\n",
            "Note:   \n",
            "\n",
            "Zero-tolerance policy for plagiarism on screening test. \n",
            "\n",
            "Please only submit your assignment via a GitHub link, any other forms      of submission will be auto rejected. \n",
            "\n",
            "Please do not send LinkedIn Requests to Connect! \n",
            "\n",
            "Internship - Seeking Data Scientist Intern that prioritizes work/life balance (Masters/Ph.D. candidates  encouraged) 2020 and later graduates. \n",
            "\n",
            "Job Description:   \n",
            "\n",
            "We are looking to hire incredible Data Scientists interested in working with a US Startup. If you are truly  passionate about designing and building machine learning solutions using python, you’re looking for a  job where you can work from anywhere- and we mean anywhere and are excited about gaining  experience in a Startup, then this is the position for you. Be it your next vacation spot or a farm out in  the country, if you have working internet, you can work remotely from your chosen location. No long  commutes or rushing to in-person meetings. Ready to work hard and play harder? Let’s work together.   \n",
            "\n",
            "Requirements:  \n",
            "\n",
            "Student in Statistics/Math, Computer Science, Finance/Economics, or related quantitative fields  (Ph.D. candidates encouraged to apply)  \n",
            "\n",
            "Experience using ML libraries, such as Scikit-learn, caret, NLTK or spark MLlib • Excellent Software skills (proficiency in Python - Data Science stack)   \n",
            "\n",
            "Build statistical and ML models to evaluate the historical performance and define predictive and  prescriptive solutions  \n",
            "\n",
            "Data Engineering experience/coursework and ability to integrate multiple data sources • Knowledge of SQL and relational Databases  \n",
            "\n",
            "Comfortable with UNIX Operating System  \n",
            "\n",
            "Knowledge of Docker and Git  \n",
            "\n",
            "Strong analytical, design, problem-solving, and troubleshooting/debugging skills • \n",
            "\n",
            "Ability to work independently in a home office and doesn’t need hand holding • \n",
            "\n",
            "Flexible schedule offered, but you will work evenings most days post your college (anywhere  between ~3-5 hours daily)  \n",
            "\n",
            "What You’ll Do, but not limited to:  \n",
            "\n",
            "Under the general direction of senior researchers, conduct empirical research using public and  proprietary data  \n",
            "\n",
            "Complete data preparation, data testing using Python (ETL), and a big part of dayto-day  function  \n",
            "\n",
            "Utilize statistical techniques to help develop analytic insights, sound hypotheses, and informed  recommendations  \n",
            "\n",
            "Conduct ad hoc quantitative analyses, modeling, or programming using Python and SQL • Diagnosing and resolving issues in production  \n",
            "\n",
            "Enhancing and developing all aspects of the company’s technology suite by collaborating with  development teams to determine application requirements  \n",
            "\n",
            "Assessing and prioritizing client feature requests \n",
            "\n",
            "Who you are:  \n",
            "\n",
            "Highly Quantitative - your skills are the envy of all your friends  \n",
            "\n",
            "Reliable, Independent, and able to wear multiple hats  \n",
            "\n",
            "Honest - we hold transparency with high regard  \n",
            "\n",
            "Team Player - likes collaborating and working as a team  \n",
            "\n",
            "Communicative - Strong verbal and written communication skills  \n",
            "\n",
            "Self-Starter - able to take ownership of projects and identify what needs to be done  \n",
            "\n",
            "Builder - You are passionate about delivering better products/experiences to our customers and  have a deep sense of ownership for your work  \n",
            "\n",
            "Experimental - You love trying new things, new tools, techniques and approaches even if you fail  sometimes  \n",
            "\n",
            "Nice to Have:  \n",
            "\n",
            "Pursuing/Completed Ph.D./Masters in Finance, Economics or Business  \n",
            "\n",
            "Have built quantitative models either in course work or in other internships is a plus  \n",
            "\n",
            "Intellectually curious and eager to learn about Finance (Investments and \n",
            "\n",
            "Corporate Finance)  \n",
            "\n",
            "Prior internship experience in Financial Services  \n",
            "\n",
            "Benefits You’ll Love:  \n",
            "\n",
            "Remote First Company - 100% remote to work with your schedule  \n",
            "\n",
            "Flexible Hours   \n",
            "\n",
            "Competitive Stipend/Salary\n"
          ]
        }
      ],
      "source": [
        "print(desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twzvXOWKq0zF"
      },
      "outputs": [],
      "source": [
        "content=[desc,resume]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glx5ItSTq-Ch"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv= CountVectorizer()\n",
        "result=cv.fit_transform(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4SSVhjKrWA0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_result=cosine_similarity(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XloDu5jrtRS",
        "outputId": "f5a4168b-e979-476d-a70f-47bf4a7421b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.33408841]\n",
            " [0.33408841 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(similarity_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0R7vkEntOdS",
        "outputId": "637322cd-6d84-414c-d283-7d45d13364ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resume matches by:0.3340884122042932\n"
          ]
        }
      ],
      "source": [
        "print('Resume matches by:'+str(similarity_result[1][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sB8ir6YsZRk"
      },
      "outputs": [],
      "source": [
        "desc1=docx2txt.process('/content/Roni Analytics - JD - DE.docx')\n",
        "resume1=docx2txt.process('/content/CURRICULAM VITAE (1).docx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRwu3rfQsiNl"
      },
      "outputs": [],
      "source": [
        "content1=[desc1,resume1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0TFkrNtsmCl"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv1= CountVectorizer()\n",
        "result1=cv1.fit_transform(content1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdyg8KjMsoyj"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_result1=cosine_similarity(result1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt5a_dfXsrit",
        "outputId": "941e7fd8-5ca0-437c-cc19-0685b7590d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.29998317]\n",
            " [0.29998317 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(similarity_result1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRt-RfAws9TN",
        "outputId": "7fac7e51-1de3-40d5-8b60-77787d731d03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resume matches by:0.2999831679932161\n"
          ]
        }
      ],
      "source": [
        "print('Resume matches by:'+str(similarity_result1[1][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXTZxD2Vt5OF"
      },
      "source": [
        "# Chatbot to Book Hotel Rooms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8vv7R_MTuAPW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('/content/ManuscriptonChatbotadoption.txt','r',errors='ignore')\n",
        "doc=f.read()\n"
      ],
      "metadata": {
        "id": "D81p0HZMkyyf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "KDhrOUo0m681",
        "outputId": "a636d884-f905-4c4d-a277-805a069d5d07"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nChatbot \\nChatbot Intelligence \\nA vast number of chatbots are being built to utilize decision-tree logic, so the response a bot gives depends on keywords identified in the users input. The bot takes the user through a conversation route based on what he or she has asked (Skerrett, 2017). Any chatbot unable to perform this function is doomed to failure. This basic function of chatbots seems very obvious and easy to implement, but it is in fact quite hard to achieve, which can be a major hurdle to chatbot adoption. Only an intelligent chatbot understands and replies to the input query in such a way that users cannot tell a robot from a real customer service operator. An intelligent machine can perform functions like 1) arithmetic; 2) comparison, logic, and reasoning; 3) learning, heuristics, and memorizing; and 5) sensing and perceiving (Khanna et al., 2015). The functioning of chatbots with respect to their application in the tourism industry is presented in Figure 1. It is an algorithm to understand possible backend structure and process to design and implement tourism bot. \\n \\nFor clarity, a new model was developed pertinent to this study. The algorithm (Figure 1) starts with the users input query in the chatbots main interface. The message is filtered through the chatbots engine and central processing. All or some of the functions can mimic human intelligence and look like a human answer. The engine and connects with the companys database, which is updated constantly to produce reliable information, such as the latest information about vacant hotel rooms, rates, or available flights. The machine can offer this information to the user instantaneously.  In the last step bot gives appropriate answer to the user.  Each step in the process refines output reply and produces most appropriate answer satisfying user needs and problems, closest to real human customer service response.  \\n \\n \\n\\n \\n\\n \\n \\nFig. 2. Sample conversation with a hotel booking chatbot \\n \\nThe direct application of chatbots in the restaurant business can be very impactful as well. In fact, Taco Bell in 2016 launched TacoBot that facilitates food ordering and recommends items while providing witty responses. Other restaurants and fast food giants like Burger King, Pizza Hut, and Dominos have followed suit with their own proprietary chatbots (Moharana, 2017). Soon placing delivery orders over the phone will be obsolete; customers will do this through Facebook, WhatsApp, or other social networking sites as shown in Figure 3. Chatbots will eventually accept payments as well; MasterCard already provides such services through its Masterpass app.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 3. Sample conversation with restaurant chatbot    \\n \\nDeploying chatbots can reduce costs for both customers and firms. Customers do not need to call, which reduces their communication expenditures, and companies will no longer need to hire customer service representatives or outsource answering services to a call center facility (Ukpabi et al., 2018). The advantages are not limited to the ordering and delivering processes. Other possible chatbot benefits Gamanyuk (2017) highlights include the allowing customers to perform the following tasks without having to download mobile apps: 1) find and explore restaurant reviews, photos, menus, prices, and available tables; 2) manage restaurant reservations on the go, easily book, change, cancel, or re-book tables; and 3) search and find restaurants according to party size, date, time, preferred cuisine, price, or distance.  \\n \\nChatbots in the Airline Industry \\nCustomer service in the airline industry is one of the first areas that could benefit from chatbots as a result of high volume of customer contact through inquiries and bookings. A good customer service bot could save money by automating tasks and unclogging call centers. It could help customers find suitable flight options by gathering information like time, date, destination, and other preferences (Agostinho, 2016). It could help in flight booking, saving customers the trouble of visiting the airlines website and entering page after page of information. It could give status updates about flights, such as information about delays or cancellations. It could also provide digital boarding passes, a service Turkish Airlines has begun to provide; offer baggage information; and gather feedback. It is reported that its introduction has recorded a huge surge in online booking (Singapore Chabots, 2017). Figure 4 shows an example of a conversation pattern with a flight booking chatbot. Normally, at the start of the conversation chatbots provide options in an easy-to-use chat interface.   \\n \\n \\n \\n \\n \\nFig. 4. Sample conversation with a flight booking chatbot    \\n \\nChatbot Challenges \\nAlthough AI and chatbots have created excitement in the tourism and hospitality industry, many concerns and problems can affect their adoption. The medias portrayal of AI as being capable of handling much of tasks in the tourism and hospitality industry is at times overrated. The rush toward chatbots is partly due to the popularity of several new messaging services. The challenges with chatbot adoption involve technical issues, cost, culture, and organization size. One of the most significant technical issues is language processing. Chatbots still commonly struggle with lexical and semantic ambiguity. Other problems are more specific to chatbots themselves, such as controlling the global course of the conversation, controlling repeated sentences, and treating unclear sentences appropriately. Such problems require adequate solutions so that chatbots can reach performance levels close to those of humans (Neves, Barros & Hodges, 2006). The cost of acquisition and setup can also be a major challenge. Murphy, Hofacker & Gretzel (2017) argue that firms will be reluctant to adopt such technology if the cost of acquisition and setup is more than they can afford. Finally, while chatbot adoption may be easy for large \\n \\n \\n \\t \\n \\n\\nFig. 5. Conceptual framework of chatbot adoption \\n \\nThe future of chatbot is essentially here; this technology has recently witnessed rapid diffusion in many sectors. Basic version of chatbots are currently utilized, which usually start conversations with easy automated options for customers and offer basic service like ordering or booking. However, fully functional chatbots that will be able to replace customer service personnel will likely become more widespread by 2020, with AI bots powering 85% of all customer service interactions (Tonner, 2016).  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWelcome to our happy holidays :\\n May I know your name:\\nMobile number:\\nCity:\\nNumber of room:\\nAddress:\\nId proof:\\nDate of arrival:\\nFeed back:\\nThank you ! visit again:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc= doc.lower()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('cmw.1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCAbO_-1oRv0",
        "outputId": "9a7590e6-7cad-4928-a679-1321ce4f4e5f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Error loading cmw.1.4: Package 'cmw.1.4' not found in\n",
            "[nltk_data]     index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "e08wcVm-qItR",
        "outputId": "233121b7-7087-46df-f756-87902cee77e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nchatbot \\nchatbot intelligence \\na vast number of chatbots are being built to utilize decision-tree logic, so the response a bot gives depends on keywords identified in the users input. the bot takes the user through a conversation route based on what he or she has asked (skerrett, 2017). any chatbot unable to perform this function is doomed to failure. this basic function of chatbots seems very obvious and easy to implement, but it is in fact quite hard to achieve, which can be a major hurdle to chatbot adoption. only an intelligent chatbot understands and replies to the input query in such a way that users cannot tell a robot from a real customer service operator. an intelligent machine can perform functions like 1) arithmetic; 2) comparison, logic, and reasoning; 3) learning, heuristics, and memorizing; and 5) sensing and perceiving (khanna et al., 2015). the functioning of chatbots with respect to their application in the tourism industry is presented in figure 1. it is an algorithm to understand possible backend structure and process to design and implement tourism bot. \\n \\nfor clarity, a new model was developed pertinent to this study. the algorithm (figure 1) starts with the users input query in the chatbots main interface. the message is filtered through the chatbots engine and central processing. all or some of the functions can mimic human intelligence and look like a human answer. the engine and connects with the companys database, which is updated constantly to produce reliable information, such as the latest information about vacant hotel rooms, rates, or available flights. the machine can offer this information to the user instantaneously.  in the last step bot gives appropriate answer to the user.  each step in the process refines output reply and produces most appropriate answer satisfying user needs and problems, closest to real human customer service response.  \\n \\n \\n\\n \\n\\n \\n \\nfig. 2. sample conversation with a hotel booking chatbot \\n \\nthe direct application of chatbots in the restaurant business can be very impactful as well. in fact, taco bell in 2016 launched tacobot that facilitates food ordering and recommends items while providing witty responses. other restaurants and fast food giants like burger king, pizza hut, and dominos have followed suit with their own proprietary chatbots (moharana, 2017). soon placing delivery orders over the phone will be obsolete; customers will do this through facebook, whatsapp, or other social networking sites as shown in figure 3. chatbots will eventually accept payments as well; mastercard already provides such services through its masterpass app.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nfig. 3. sample conversation with restaurant chatbot    \\n \\ndeploying chatbots can reduce costs for both customers and firms. customers do not need to call, which reduces their communication expenditures, and companies will no longer need to hire customer service representatives or outsource answering services to a call center facility (ukpabi et al., 2018). the advantages are not limited to the ordering and delivering processes. other possible chatbot benefits gamanyuk (2017) highlights include the allowing customers to perform the following tasks without having to download mobile apps: 1) find and explore restaurant reviews, photos, menus, prices, and available tables; 2) manage restaurant reservations on the go, easily book, change, cancel, or re-book tables; and 3) search and find restaurants according to party size, date, time, preferred cuisine, price, or distance.  \\n \\nchatbots in the airline industry \\ncustomer service in the airline industry is one of the first areas that could benefit from chatbots as a result of high volume of customer contact through inquiries and bookings. a good customer service bot could save money by automating tasks and unclogging call centers. it could help customers find suitable flight options by gathering information like time, date, destination, and other preferences (agostinho, 2016). it could help in flight booking, saving customers the trouble of visiting the airlines website and entering page after page of information. it could give status updates about flights, such as information about delays or cancellations. it could also provide digital boarding passes, a service turkish airlines has begun to provide; offer baggage information; and gather feedback. it is reported that its introduction has recorded a huge surge in online booking (singapore chabots, 2017). figure 4 shows an example of a conversation pattern with a flight booking chatbot. normally, at the start of the conversation chatbots provide options in an easy-to-use chat interface.   \\n \\n \\n \\n \\n \\nfig. 4. sample conversation with a flight booking chatbot    \\n \\nchatbot challenges \\nalthough ai and chatbots have created excitement in the tourism and hospitality industry, many concerns and problems can affect their adoption. the medias portrayal of ai as being capable of handling much of tasks in the tourism and hospitality industry is at times overrated. the rush toward chatbots is partly due to the popularity of several new messaging services. the challenges with chatbot adoption involve technical issues, cost, culture, and organization size. one of the most significant technical issues is language processing. chatbots still commonly struggle with lexical and semantic ambiguity. other problems are more specific to chatbots themselves, such as controlling the global course of the conversation, controlling repeated sentences, and treating unclear sentences appropriately. such problems require adequate solutions so that chatbots can reach performance levels close to those of humans (neves, barros & hodges, 2006). the cost of acquisition and setup can also be a major challenge. murphy, hofacker & gretzel (2017) argue that firms will be reluctant to adopt such technology if the cost of acquisition and setup is more than they can afford. finally, while chatbot adoption may be easy for large \\n \\n \\n \\t \\n \\n\\nfig. 5. conceptual framework of chatbot adoption \\n \\nthe future of chatbot is essentially here; this technology has recently witnessed rapid diffusion in many sectors. basic version of chatbots are currently utilized, which usually start conversations with easy automated options for customers and offer basic service like ordering or booking. however, fully functional chatbots that will be able to replace customer service personnel will likely become more widespread by 2020, with ai bots powering 85% of all customer service interactions (tonner, 2016).  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nwelcome to our happy holidays :\\n may i know your name:\\nmobile number:\\ncity:\\nnumber of room:\\naddress:\\nid proof:\\ndate of arrival:\\nfeed back:\\nthank you ! visit again:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens=nltk.sent_tokenize(doc)\n",
        "word_tokens=nltk.word_tokenize(doc)"
      ],
      "metadata": {
        "id": "rmE0ioJJqN_u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens[:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR6SpDFsqlrl",
        "outputId": "38b3f520-854b-43db-82cc-146eeb4bcefc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n\\nchatbot \\nchatbot intelligence \\na vast number of chatbots are being built to utilize decision-tree logic, so the response a bot gives depends on keywords identified in the users input.',\n",
              " 'the bot takes the user through a conversation route based on what he or she has asked (skerrett, 2017).',\n",
              " 'any chatbot unable to perform this function is doomed to failure.',\n",
              " 'this basic function of chatbots seems very obvious and easy to implement, but it is in fact quite hard to achieve, which can be a major hurdle to chatbot adoption.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvi7CYjtqtk8",
        "outputId": "551e2d44-4d59-4151-fbc9-79ee28e3d034"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chatbot', 'chatbot', 'intelligence', 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer=nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return[lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punc_dict= dict((ord(punct),None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
      ],
      "metadata": {
        "id": "vNAhUTDCqzUa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greet_inputs=('hello','hi','welcome','How can i help you?')\n",
        "greet_responses=('hi','hey','hello back','hello')\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)"
      ],
      "metadata": {
        "id": "nc7c71ZVtlkL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "zyomMhmawp9F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "  robot_response=\"\"\n",
        "  TfidfVec=TfidfVectorizer(tokenizer=LemNormalize,stop_words='english')\n",
        "  tfidf=TfidfVec.fit_transform(sentence_tokens)\n",
        "  vals=cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx=vals.argsort()[0][-2]\n",
        "  flat=vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf=flat[-2]\n",
        "  if(req_tfidf==0):\n",
        "    robot_response=robot_response+'Iam sorry,unable to understand you'\n",
        "    return robot_response\n",
        "  else:\n",
        "    robot_response=robot_response+sentence_tokens[idx]\n",
        "    return robot_response"
      ],
      "metadata": {
        "id": "SQaO-1aFxEJL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag=True\n",
        "print('Hello I am the chatbot how can i help you? .For ending convo type bye')\n",
        "while (flag==True):\n",
        "  user_response=input()\n",
        "  user_response=user_response.lower()\n",
        "  if(user_response!='bye'):\n",
        "    if(user_response=='thankyou'or user_response=='thanks'):\n",
        "      flag=False\n",
        "      print('Bot:You are welcome....')\n",
        "    else:\n",
        "      if(greet(user_response)!=None):\n",
        "        print('Bot'+greet(user_response))\n",
        "      else:\n",
        "        sentence_tokens.append(user_response)\n",
        "        word_tokens=word_tokens+nltk.wordpunct_tokenize(user_response)\n",
        "        final_words=list(set(word_tokens))\n",
        "        print('Bot:',end=\"\")\n",
        "        print(response(user_response))\n",
        "        sentence_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag=False\n",
        "    print('Bot:Goodbye!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50vAY-5ezQoa",
        "outputId": "56c94bdc-7972-4d7e-afc4-6b33afca9aad"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello I am the chatbot how can i help you? .For ending convo type bye\n",
            "bye\n",
            "Bot:Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGyV1isxuUCC"
      },
      "source": [
        "# Spell Corrector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c_ettYjzsz_",
        "outputId": "37332eca-d3c9-496b-cc93-dda79b610f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.8/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.8/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JshLcczx0ArH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "7b37ac79-2c6f-4247-b852-ee4a27428ceb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c6191829e0df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mcorrected_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mcorrected_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spelling checking'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'400x200'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2259\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import matplotlib as mpl\n",
        "#if os.environ.get('DISPLAY','') == '':\n",
        "    #print('no display found. Using non-interactive Agg backend')\n",
        "    #mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "from textblob import TextBlob \n",
        "from tkinter import *\n",
        "\n",
        "def checking():\n",
        "  a=TextBlob(k.get())\n",
        "  corrected_text=Label(root,text=str(a.correct()))\n",
        "  corrected_text.pack()\n",
        "root =Tk()\n",
        "root.title('spelling checking')\n",
        "root.geometry('400x200')\n",
        "\n",
        "head=Label(root,text='spelling checking',font=('helvetica',14,'bold'))\n",
        "head.pack()\n",
        "k=Entry(root,width=200,borderwidth=5)\n",
        "k.pack()\n",
        "a=Button(root,text='checking',font=('helvetica',8,'bold'),fg='white',bg='brown',command=checking)\n",
        "a.pack()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOc1YlU2CigBx/0Nyfw67Bl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}